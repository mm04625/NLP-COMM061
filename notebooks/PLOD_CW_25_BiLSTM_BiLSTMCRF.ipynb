{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xYU7xgIsv1Jh"
      },
      "outputs": [],
      "source": [
        "# ! pip install -U datasets torch transformers\n",
        "# ! pip install torchcrf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lBWhZ57uDawg"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW-25\")\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9W7cwbDODcmq"
      },
      "outputs": [],
      "source": [
        "all_tokens = list(chain(*train_data[\"tokens\"]))\n",
        "all_tags = list(chain(*train_data[\"ner_tags\"]))\n",
        "\n",
        "token2idx = {tok: i+2 for i, tok in enumerate(set(all_tokens))}\n",
        "token2idx[\"<PAD>\"] = 0\n",
        "token2idx[\"<UNK>\"] = 1\n",
        "\n",
        "tag_set = sorted(set(all_tags))\n",
        "tag2idx = {tag: i for i, tag in enumerate(tag_set)}\n",
        "idx2tag = {i: tag for tag, i in tag2idx.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZI46QwtBDmjD"
      },
      "outputs": [],
      "source": [
        "def encode_and_pad(data, pad_token_id=0, pad_label_id=-100):\n",
        "    inputs, labels = [], []\n",
        "    for tokens, tags in zip(data[\"tokens\"], data[\"ner_tags\"]):\n",
        "        input_ids = [token2idx.get(tok, token2idx[\"<UNK>\"]) for tok in tokens]\n",
        "        label_ids = [tag2idx[tag] for tag in tags]\n",
        "        inputs.append(input_ids)\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    max_len = max(len(seq) for seq in inputs)\n",
        "    inputs = [seq + [pad_token_id]*(max_len - len(seq)) for seq in inputs]\n",
        "    labels = [seq + [pad_label_id]*(max_len - len(seq)) for seq in labels]\n",
        "    return inputs, labels\n",
        "\n",
        "train_inputs, train_labels = encode_and_pad(train_data)\n",
        "test_inputs, test_labels = encode_and_pad(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "rjSg5HP7Dm9w"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NERDataset(train_inputs, train_labels)\n",
        "test_dataset = NERDataset(test_inputs, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OheVreqoDpvW"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=32, hidden_dim=64):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeds = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "model = BiLSTMTagger(len(token2idx), len(tag2idx))\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aKaPHRiaDrvE"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            labels = batch[\"labels\"]\n",
        "            logits = model(input_ids)\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            for p, l in zip(preds, labels):\n",
        "                p = p.cpu().numpy()\n",
        "                l = l.cpu().numpy()\n",
        "                valid = l != -100\n",
        "                all_preds.extend(p[valid])\n",
        "                all_labels.extend(l[valid])\n",
        "    model.train()\n",
        "    return f1_score(all_labels, all_preds, average=\"macro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmGyZV3CDvqo",
        "outputId": "92a325fe-06e9-43bb-861c-1b239035abd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 56.3071, Test F1: 0.2198, Time: 12.94s\n",
            "Epoch 2, Loss: 37.6807, Test F1: 0.5458, Time: 8.76s\n",
            "Epoch 3, Loss: 29.0124, Test F1: 0.6614, Time: 6.32s\n",
            "Epoch 4, Loss: 24.4965, Test F1: 0.7166, Time: 5.54s\n",
            "Epoch 5, Loss: 21.4510, Test F1: 0.7089, Time: 6.37s\n",
            "Epoch 6, Loss: 18.8487, Test F1: 0.7322, Time: 5.57s\n",
            "Epoch 7, Loss: 16.6470, Test F1: 0.7243, Time: 6.36s\n",
            "Epoch 8, Loss: 14.7942, Test F1: 0.7561, Time: 5.53s\n",
            "Epoch 9, Loss: 13.0917, Test F1: 0.7417, Time: 6.38s\n",
            "Epoch 10, Loss: 11.4056, Test F1: 0.7392, Time: 5.59s\n",
            "Epoch 11, Loss: 9.9521, Test F1: 0.7323, Time: 6.16s\n",
            "Epoch 12, Loss: 8.6873, Test F1: 0.7421, Time: 5.72s\n",
            "Epoch 13, Loss: 7.3983, Test F1: 0.7284, Time: 5.84s\n",
            "Epoch 14, Loss: 6.3146, Test F1: 0.7425, Time: 6.07s\n",
            "Epoch 15, Loss: 5.4538, Test F1: 0.7260, Time: 6.63s\n",
            "Epoch 16, Loss: 4.7362, Test F1: 0.7450, Time: 6.16s\n",
            "Epoch 17, Loss: 4.0168, Test F1: 0.7313, Time: 5.59s\n",
            "Epoch 18, Loss: 3.4042, Test F1: 0.7362, Time: 6.34s\n",
            "Epoch 19, Loss: 2.9285, Test F1: 0.7403, Time: 5.59s\n",
            "Epoch 20, Loss: 2.4658, Test F1: 0.7328, Time: 6.35s\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(20):\n",
        "    start_time = time.time()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = loss_fn(logits.view(-1, len(tag2idx)), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    test_f1 = evaluate(model, test_loader)\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Test F1: {test_f1:.4f}, Time: {duration:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -U TorchCRF"
      ],
      "metadata": {
        "id": "1w4ViPJ2V6Ow"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mog6URUI_-CK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from TorchCRF import CRF\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "\n",
        "token2idx = {tok: i+2 for i, tok in enumerate(set(all_tokens))}\n",
        "token2idx[\"<PAD>\"] = 0\n",
        "token2idx[\"<UNK>\"] = 1\n",
        "\n",
        "tag_set  = sorted(set(all_tags))\n",
        "tag2idx  = {tag: i for i, tag in enumerate(tag_set)}\n",
        "idx2tag  = {i: tag for tag, i in tag2idx.items()}\n",
        "\n",
        "PAD_TOKEN_ID = token2idx[\"<PAD>\"]\n",
        "PAD_LABEL_ID = tag2idx[\"O\"]\n",
        "\n",
        "train_inputs, train_labels = encode_and_pad(train_data, PAD_TOKEN_ID, PAD_LABEL_ID)\n",
        "test_inputs,  test_labels  = encode_and_pad(test_data,  PAD_TOKEN_ID, PAD_LABEL_ID)\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels    = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "          \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "          \"labels\":    torch.tensor(self.labels[idx],    dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_loader = DataLoader(NERDataset(train_inputs, train_labels), batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(NERDataset(test_inputs,  test_labels),  batch_size=32)\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size,\n",
        "                 embedding_dim=32, hidden_dim=64,\n",
        "                 pad_token_id=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim,\n",
        "                                      padding_idx=pad_token_id)\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim // 2,\n",
        "                            num_layers=1,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True)\n",
        "        self.fc  = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.crf = CRF(tagset_size)\n",
        "\n",
        "    def _get_emissions(self, input_ids):\n",
        "        x, _ = self.lstm(self.embedding(input_ids))\n",
        "        return self.fc(x)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        emissions = self._get_emissions(input_ids)\n",
        "        mask      = (input_ids != self.embedding.padding_idx)\n",
        "        return self.crf.viterbi_decode(emissions, mask)\n",
        "\n",
        "    def loss(self, input_ids, tags):\n",
        "        emissions = self._get_emissions(input_ids)\n",
        "        mask      = (input_ids != self.embedding.padding_idx)\n",
        "        log_likelihood = self.crf.forward(emissions, tags, mask)\n",
        "        return -log_likelihood.mean()\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds_all, labels_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            labels    = batch[\"labels\"]\n",
        "            batch_preds = model(input_ids)\n",
        "            for pred_seq, label_seq, inp_seq in zip(batch_preds,\n",
        "                                                    labels,\n",
        "                                                    input_ids):\n",
        "                valid_len = (inp_seq != PAD_TOKEN_ID).sum().item()\n",
        "                preds_all.extend(pred_seq[:valid_len])\n",
        "                labels_all.extend(label_seq[:valid_len].tolist())\n",
        "    model.train()\n",
        "    return f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crf_model = BiLSTM_CRF(len(token2idx), len(tag2idx), pad_token_id=PAD_TOKEN_ID)\n",
        "optimizer = torch.optim.Adam(crf_model.parameters(), lr=2e-3)\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    total_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = crf_model.loss(batch[\"input_ids\"], batch[\"labels\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    test_f1 = evaluate(crf_model, test_loader)\n",
        "    print(f\"Epoch {epoch:2d} | Loss: {total_loss:.4f} | Test F1: {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pfq0YYyXRdB",
        "outputId": "4b8610c5-19c6-4f73-8637-742b99b60a70"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | Loss: 2238.0042 | Test F1: 0.2652\n",
            "Epoch  2 | Loss: 1492.8622 | Test F1: 0.5512\n",
            "Epoch  3 | Loss: 1132.5842 | Test F1: 0.6690\n",
            "Epoch  4 | Loss: 946.1746 | Test F1: 0.7211\n",
            "Epoch  5 | Loss: 821.9778 | Test F1: 0.7436\n",
            "Epoch  6 | Loss: 715.2990 | Test F1: 0.7514\n",
            "Epoch  7 | Loss: 629.1771 | Test F1: 0.7623\n",
            "Epoch  8 | Loss: 548.8198 | Test F1: 0.7655\n",
            "Epoch  9 | Loss: 480.3982 | Test F1: 0.7662\n",
            "Epoch 10 | Loss: 417.0347 | Test F1: 0.7686\n",
            "Epoch 11 | Loss: 358.6615 | Test F1: 0.7634\n",
            "Epoch 12 | Loss: 309.1900 | Test F1: 0.7620\n",
            "Epoch 13 | Loss: 263.8379 | Test F1: 0.7580\n",
            "Epoch 14 | Loss: 223.4335 | Test F1: 0.7615\n",
            "Epoch 15 | Loss: 191.0393 | Test F1: 0.7540\n",
            "Epoch 16 | Loss: 162.8920 | Test F1: 0.7470\n",
            "Epoch 17 | Loss: 138.7516 | Test F1: 0.7484\n",
            "Epoch 18 | Loss: 128.8066 | Test F1: 0.7475\n",
            "Epoch 19 | Loss: 102.0246 | Test F1: 0.7496\n",
            "Epoch 20 | Loss: 84.0187 | Test F1: 0.7464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLV15bj9AD6M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YauTzDfVAaCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}